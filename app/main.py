import os
import time
import boto3
import json
import streamlit as st
from botocore.exceptions import ClientError
from langchain_aws import BedrockLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.document_loaders import PubMedLoader

# AWSèªè¨¼æƒ…å ±ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
os.environ['AWS_ACCESS_KEY_ID'] = st.secrets["AWS_ACCESS_KEY_ID"]
os.environ['AWS_SECRET_ACCESS_KEY'] = st.secrets["AWS_SECRET_ACCESS_KEY"]
os.environ['AWS_DEFAULT_REGION'] = st.secrets["AWS_DEFAULT_REGION"]
os.environ['AWS_PROFILE'] = 'default'

def main():
    st.title("ç”ŸæˆAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°Appé›†")
    st.warning("è©¦ä½œå“ã«ã¤ãã€å“è³ªã®ä¿è¨¼ã¯ã‚ã‚Šã¾ã›ã‚“", icon="ğŸš¨")
    st.divider()

    mode = st.sidebar.radio("ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹", ["ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰", "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ", "æ–‡çŒ®æ¤œç´¢ãƒ»è¦ç´„ãƒ¢ãƒ¼ãƒ‰"])

    if mode == "ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰":
        research_mode()
    elif mode == "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ":
        simplechat_mode()
    elif mode == "æ–‡çŒ®æ¤œç´¢ãƒ»è¦ç´„ãƒ¢ãƒ¼ãƒ‰":
        literature_search_mode()

def research_mode():
    st.header("ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰")
    
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.chat_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.chat_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.chat_messages.append({"role": "assistant", "content": response})

def simplechat_mode():
    st.header("ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ")
    
    if "dev_messages" not in st.session_state:
        st.session_state.dev_messages = []

    for message in st.session_state.dev_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.dev_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.dev_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.dev_messages.append({"role": "assistant", "content": response})

def literature_search_mode():
    st.header("æ–‡çŒ®æ¤œç´¢ãƒ»è¦ç´„ãƒ¢ãƒ¼ãƒ‰")

    llm = BedrockLLM(credentials_profile_name="default", model_id="anthropic.claude-v2:1")

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""Pubmedã§æ¬¡ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ãŸã„ã®ã§ã™ãŒã€ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚

è‰¯ã„ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: COVID-19 AND (å®¶æ—å†…ä¼æ’­ OR household transmission)

é–“é•ã„ã®ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: ä»¥ä¸‹ãŒç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã§ã™ï¼šCOVID-19 AND household transmission

ã‚¯ã‚¨ãƒª: {query}
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    summary_prompt = PromptTemplate(
        input_variables=["text"],
        template="ä»¥ä¸‹ã®åŒ»å­¦è«–æ–‡ã®è¦ç´„ã‚’100å˜èªä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
    )

    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„åŒ»å­¦ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=10, value=5)

    if st.button("æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            loader = PubMedLoader(query=optimized_query, load_max_docs=max_docs)
            docs = loader.load()

            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                content = doc.page_content

                with st.spinner(f'æ–‡çŒ® {i} ã‚’è¦ç´„ä¸­...'):
                    llm_summary = summary_chain.run(content).strip()

                st.subheader(f"æ–‡çŒ® {i}")
                st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {metadata.get('Title', 'ä¸æ˜')}")
                st.write(f"è‘—è€…: {metadata.get('Authors', 'ä¸æ˜')}")
                st.write(f"å‡ºç‰ˆæ—¥: {metadata.get('Published', 'ä¸æ˜')}")
                st.write(f"PMID: {metadata.get('uid', 'ä¸æ˜')}")
                with st.expander("æ¦‚è¦"):
                    st.write(content[:500] + "..." if len(content) > 500 else content)
                st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                st.write(llm_summary)
                st.divider()

def generate_text(messages):
    client = boto3.client("bedrock-runtime", region_name="ap-northeast-1")
    model_id = "anthropic.claude-v2:1"

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    formatted_messages = []
    for msg in messages:
        if msg["role"] == "user":
            formatted_messages.append({"role": "human", "content": msg["content"]})
        elif msg["role"] == "assistant":
            formatted_messages.append({"role": "assistant", "content": msg["content"]})

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ä½œæˆ
    request_body = {
        "prompt": "\n\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in formatted_messages]) + "\n\nAssistant:",
        "max_tokens_to_sample": 500,
        "temperature": 0.7,
        "top_p": 1,
    }

    request = json.dumps(request_body)

    try:
        response = client.invoke_model(modelId=model_id, body=request)
        response_body = json.loads(response["body"].read())
        return response_body.get("completion", "No response generated.")
    except ClientError as e:
        print(f"An error occurred: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼é¢¨ã®æ¼”å‡º
def stream_data(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.05)

if __name__ == "__main__":
    main()