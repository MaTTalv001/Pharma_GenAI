import os
import pandas as pd
import time
import boto3
import json
import streamlit as st
from botocore.exceptions import ClientError
from langchain_aws import BedrockLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationalRetrievalChain
from langchain_community.document_loaders import PubMedLoader
from langchain_community.retrievers import WikipediaRetriever
from langchain.chains import RetrievalQA
from langchain_community.utilities import ArxivAPIWrapper
import arxiv

# AWSèªè¨¼æƒ…å ±ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
os.environ['AWS_ACCESS_KEY_ID'] = st.secrets["AWS_ACCESS_KEY_ID"]
os.environ['AWS_SECRET_ACCESS_KEY'] = st.secrets["AWS_SECRET_ACCESS_KEY"]
os.environ['AWS_DEFAULT_REGION'] = st.secrets["AWS_DEFAULT_REGION"]
os.environ['AWS_PROFILE'] = 'default'

def main():
    st.title("ç”ŸæˆAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°Appé›†")
    
    st.divider()

    mode = st.sidebar.radio("ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹", ["ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰", "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ", "PubMedæ¤œç´¢ãƒ»è¦ç´„", "wikiæ¤œç´¢", "arxivæ¤œç´¢"])
    st.sidebar.warning("è©¦ä½œå“ã«ã¤ãã€å“è³ªã®ä¿è¨¼ã¯ã‚ã‚Šã¾ã›ã‚“", icon="ğŸš¨")

    if mode == "ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰":
        research_mode()
    elif mode == "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ":
        simplechat_mode()
    elif mode == "PubMedæ¤œç´¢ãƒ»è¦ç´„":
        pubmed_search_mode()
    elif mode == "wikiæ¤œç´¢":
        wiki_search_mode()
    elif mode == "arxivæ¤œç´¢":
        arxiv_search_mode()

def research_mode():
    st.header("ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰")
    
    
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.chat_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.chat_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.chat_messages.append({"role": "assistant", "content": response})

def simplechat_mode():
    st.header("ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ")
    st.info("é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚LLMã®äº‹å‰å­¦ç¿’å†…å®¹ã‚’å‚ç…§ã—ã¦å›ç­”ã•ã‚Œã¾ã™ã€‚", icon=None)
    
    if "dev_messages" not in st.session_state:
        st.session_state.dev_messages = []

    for message in st.session_state.dev_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.dev_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.dev_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.dev_messages.append({"role": "assistant", "content": response})

def pubmed_search_mode():
    st.header("PubMedæ¤œç´¢ãƒ»è¦ç´„")
    st.info("LLMã§ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã€é–¢é€£è¨˜äº‹ã‚’PubMedã‹ã‚‰æ¤œç´¢ã¨è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¡Œã„ã¾ã™", icon=None)

    summarize = st.checkbox("LLMã«ã‚ˆã‚‹è¦ç´„ã‚’è¡Œã†(ä»¶æ•°ä¸Šé™ã¯å°‘ãªããªã‚Šã¾ã™)", value=False)
    
    # if summarize:
    #     st.markdown("- LLMã‚’ç”¨ã„ã¦ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¾ã™")
    #     st.markdown("- LLMã‚’ç”¨ã„ã¦å„è«–æ–‡ã‚’è¦ç´„ã—ã¾ã™")
    # else:
    #     st.markdown("- LLMã‚’ç”¨ã„ã¦ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¾ã™")

    llm = BedrockLLM(credentials_profile_name="default", model_id="anthropic.claude-v2:1")

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""Pubmedã§æ¬¡ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ãŸã„ã®ã§ã™ãŒã€ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã€Œæœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:ã€ãªã©ã®ä½™è¨ˆãªæ–‡è¨€ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚

è‰¯ã„ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: COVID-19 AND (å®¶æ—å†…ä¼æ’­ OR household transmission)

é–“é•ã„ã®ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: ä»¥ä¸‹ãŒç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã§ã™ï¼šCOVID-19 AND household transmission

ã‚¯ã‚¨ãƒª: {query}
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„åŒ»å­¦ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=100 if not summarize else 10, value=50 if not summarize else 5)

    if st.button("æ¤œç´¢" if not summarize else "æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            optimized_query = optimized_query.split('\n')[-1].strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            loader = PubMedLoader(query=optimized_query, load_max_docs=max_docs)
            docs = loader.load()

            results = []
            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                content = doc.page_content

                pmid = metadata.get('uid', 'ä¸æ˜')
                pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid != 'ä¸æ˜' else ''

                result = {
                    "ã‚¿ã‚¤ãƒˆãƒ«": metadata.get('Title', 'ä¸æ˜'),
                    "å‡ºç‰ˆæ—¥": metadata.get('Published', 'ä¸æ˜'),
                    "PMID": pmid,
                    "PubMed URL": pubmed_url,
                    "æ¦‚è¦": content[:500] + "..." if len(content) > 500 else content
                }

                if summarize:
                    with st.spinner(f'æ–‡çŒ® {i} ã‚’è¦ç´„ä¸­...'):
                        summary_prompt = PromptTemplate(
                            input_variables=["text"],
                            template="ä»¥ä¸‹ã®åŒ»å­¦è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§3æ–‡ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
                        )
                        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
                        result["LLMã«ã‚ˆã‚‹è¦ç´„"] = summary_chain.run(content).strip()

                results.append(result)

                if summarize:
                    st.subheader(f"æ–‡çŒ® {i}")
                    st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['ã‚¿ã‚¤ãƒˆãƒ«']}")
                    st.write(f"å‡ºç‰ˆæ—¥: {result['å‡ºç‰ˆæ—¥']}")
                    if pubmed_url:
                        st.markdown(f"PMID: [{pmid}]({pubmed_url})")
                    else:
                        st.write("PMID: ä¸æ˜")
                    with st.expander("æ¦‚è¦"):
                        st.write(result['æ¦‚è¦'])
                    st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                    st.write(result['LLMã«ã‚ˆã‚‹è¦ç´„'])
                    st.divider()

            # DataFrameã®ä½œæˆã¨è¡¨ç¤º
            df = pd.DataFrame(results)
            st.subheader("æ¤œç´¢çµæœä¸€è¦§")
            st.dataframe(df[["ã‚¿ã‚¤ãƒˆãƒ«", "å‡ºç‰ˆæ—¥", "PMID", "PubMed URL"]])

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="pubmed_search_results.csv",
                mime="text/csv",
            )

            if not summarize:
                # å€‹åˆ¥ã®è«–æ–‡è©³ç´°è¡¨ç¤º
                st.subheader("è«–æ–‡è©³ç´°")
                for i, result in enumerate(results, 1):
                    with st.expander(f"è«–æ–‡ {i}: {result['ã‚¿ã‚¤ãƒˆãƒ«']}"):
                        st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['ã‚¿ã‚¤ãƒˆãƒ«']}")
                        st.write(f"å‡ºç‰ˆæ—¥: {result['å‡ºç‰ˆæ—¥']}")
                        if result['PubMed URL']:
                            st.markdown(f"PMID: [{result['PMID']}]({result['PubMed URL']})")
                        else:
                            st.write(f"PMID: {result['PMID']}")
                        st.write("æ¦‚è¦:")
                        st.write(result['æ¦‚è¦'])
def wiki_search_mode():
    
    st.title("Wikipediaæ¤œç´¢")
    st.info("Wikipediaã‹ã‚‰é–¢é€£è¨˜äº‹ã‚’æ¤œç´¢ã—ã¦å›ç­”ã‚’è¿”ã—ã¾ã™", icon=None)

    # è¨€èªé¸æŠ
    lang = st.radio("è¨€èªã‚’é¸æŠã—ã¦ãã ã•ã„:", ["æ—¥æœ¬èª", "è‹±èª"])
    lang_code = "ja" if lang == "æ—¥æœ¬èª" else "en"

    # BedrockLLMã®è¨­å®š
    llm = BedrockLLM(model_id="anthropic.claude-v2:1")

    # WikipediaRetrieverã®è¨­å®š
    retriever = WikipediaRetriever(lang=lang_code, top_k_results=5)

    # RetrievalQAã®è¨­å®š
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)

    # æ–°ã—ã„è³ªå•ã®å…¥åŠ›
    query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

    if st.button("æ¤œç´¢"):
        if query:
            with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
                result = qa({"query": query})
                answer = result['result']
                sources = result['source_documents']

                st.subheader("å›ç­”:")
                st.write(answer)
                
                st.subheader("Wikipediaè¨˜äº‹:")
                for i, source in enumerate(sources, 1):
                    with st.expander(f"è¨˜äº‹ {i}: {source.metadata.get('title', 'ä¸æ˜')}"):
                        st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {source.metadata.get('title', 'ä¸æ˜')}")
                        st.write(f"URL: {source.metadata.get('source', 'ä¸æ˜')}")
                        st.write("å†…å®¹:")
                        st.write(source.page_content)

def arxiv_search_mode():
    st.header("arXivæ¤œç´¢ãƒ»è¦ç´„")
    st.info("LLMã§ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã€é–¢é€£è«–æ–‡ã‚’arXivã‹ã‚‰æ¤œç´¢ã¨è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¡Œã„ã¾ã™", icon=None)

    summarize = st.checkbox("LLMã«ã‚ˆã‚‹è¦ç´„ã‚’è¡Œã†(ä»¶æ•°ä¸Šé™ã¯å°‘ãªããªã‚Šã¾ã™)", value=False)

    llm = BedrockLLM(credentials_profile_name="default", model_id="anthropic.claude-v2:1")

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""ä»¥ä¸‹ã®ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€arXivã§ã®æ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒª(è‹±èª)ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾arXivã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚
        
        æ­£ã—ã„ä¾‹ï¼š
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯:LLMã®æ¯”è¼ƒ
        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:Comparative analysis of language models

        æ­£ã—ã„ä¾‹ï¼š
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯:LLMã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿åˆ†æ
        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:Data analysis using language models

        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {query}

        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=20 if not summarize else 5, value=5)

    if st.button("æ¤œç´¢" if not summarize else "æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            # arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç›´æ¥ä½¿ç”¨ã—ã¦æ¤œç´¢
            search = arxiv.Search(
                query=optimized_query,
                max_results=max_docs,
                sort_by=arxiv.SortCriterion.Relevance
            )

            papers_data = []
            for i, result in enumerate(search.results(), 1):
                paper_data = {
                    "Title": result.title,
                    "Authors": ", ".join(author.name for author in result.authors),
                    "Published": result.published.strftime("%Y-%m-%d"),
                    "URL": result.entry_id,
                    "Summary": result.summary,
                    "Journal Ref": result.journal_ref,
                    "DOI": result.doi,
                }

                st.subheader(f"è«–æ–‡ {i}")
                st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {paper_data['Title']}")
                st.write(f"è‘—è€…: {paper_data['Authors']}")
                st.write(f"å‡ºç‰ˆæ—¥: {paper_data['Published']}")
                st.write(f"URL: {paper_data['URL']}")
                if paper_data['Journal Ref']:
                    st.write(f"ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å‚ç…§: {paper_data['Journal Ref']}")
                if paper_data['DOI']:
                    st.write(f"DOI: {paper_data['DOI']}")
                
                with st.expander("è¦ç´„"):
                    st.write(paper_data['Summary'])

                if summarize:
                    with st.spinner(f'è«–æ–‡ {i} ã‚’è¦ç´„ä¸­...'):
                        summary_prompt = PromptTemplate(
                            input_variables=["text"],
                            template="ä»¥ä¸‹ã®è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§3æ–‡ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
                        )
                        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
                        llm_summary = summary_chain.run(paper_data["Summary"]).strip()
                        st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                        st.write(llm_summary)
                        paper_data["LLMè¦ç´„"] = llm_summary

                papers_data.append(paper_data)
                st.divider()

            # DataFrameã®ä½œæˆã¨è¡¨ç¤º
            df = pd.DataFrame(papers_data)
            st.subheader("æ¤œç´¢çµæœä¸€è¦§")
            st.dataframe(df[["Title", "Authors", "Published", "URL"]])

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="arxiv_search_results.csv",
                mime="text/csv",
            )


def generate_text(messages):
    client = boto3.client("bedrock-runtime", region_name="ap-northeast-1")
    model_id = "anthropic.claude-v2:1"

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    formatted_messages = []
    for msg in messages:
        if msg["role"] == "user":
            formatted_messages.append({"role": "human", "content": msg["content"]})
        elif msg["role"] == "assistant":
            formatted_messages.append({"role": "assistant", "content": msg["content"]})

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ä½œæˆ
    request_body = {
        "prompt": "\n\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in formatted_messages]) + "\n\nAssistant:",
        "max_tokens_to_sample": 500,
        "temperature": 0.7,
        "top_p": 1,
    }

    request = json.dumps(request_body)

    try:
        response = client.invoke_model(modelId=model_id, body=request)
        response_body = json.loads(response["body"].read())
        return response_body.get("completion", "No response generated.")
    except ClientError as e:
        print(f"An error occurred: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼é¢¨ã®æ¼”å‡º
def stream_data(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.05)

if __name__ == "__main__":
    main()