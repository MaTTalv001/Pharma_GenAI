import os
import pandas as pd
import numpy as np
import time
import boto3
import json
import requests
import feedparser
from bs4 import BeautifulSoup
from io import BytesIO
import arxiv
import wikipedia
import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from botocore.exceptions import ClientError
from langchain_aws import BedrockLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationalRetrievalChain
from langchain_community.document_loaders import PubMedLoader
from langchain_community.retrievers import WikipediaRetriever
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import BedrockEmbeddings
from langchain.schema import Document


# AWSèªè¨¼æƒ…å ±ã®è¨­å®š
if 'aws_credentials' in st.secrets:
    os.environ['AWS_ACCESS_KEY_ID'] = st.secrets["aws_credentials"]["AWS_ACCESS_KEY_ID"]
    os.environ['AWS_SECRET_ACCESS_KEY'] = st.secrets["aws_credentials"]["AWS_SECRET_ACCESS_KEY"]
    os.environ['AWS_DEFAULT_REGION'] = st.secrets["aws_credentials"]["AWS_DEFAULT_REGION"]
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    os.environ['AWS_ACCESS_KEY_ID'] = 'your_access_key_id'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret_access_key'
    os.environ['AWS_DEFAULT_REGION'] = 'your_region'

def get_llm():
    return BedrockLLM(
        model_id="anthropic.claude-v2:1",
        region_name=os.environ['AWS_DEFAULT_REGION']
    )

def main():
    st.title("ç”ŸæˆAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°Appé›†")
    st.divider()

    mode = st.sidebar.radio("ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹", ["ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ", "ä¼ç”»ç›¸è«‡","PubMedæ¤œç´¢ãƒ»è¦ç´„", "Wikipediaæ¤œç´¢", "arxivæ¤œç´¢", "ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—"])
    st.sidebar.warning("è©¦ä½œå“ã«ã¤ãã€å“è³ªã®ä¿è¨¼ã¯ã‚ã‚Šã¾ã›ã‚“", icon="ğŸš¨")

    if mode == "ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰":
        research_mode()
    elif mode == "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ":
        simplechat_mode()
    elif mode == "PubMedæ¤œç´¢ãƒ»è¦ç´„":
        pubmed_search_mode()
    elif mode == "Wikipediaæ¤œç´¢":
        wiki_search_mode()
    elif mode == "arxivæ¤œç´¢":
        arxiv_search_mode()
    elif mode == "ä¼ç”»ç›¸è«‡":
        idea_consultation_mode()
    elif mode == "ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—":
        news_feed_mode()

def research_mode():
    st.header("ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰")
    
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.chat_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.chat_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.chat_messages.append({"role": "assistant", "content": response})

def simplechat_mode():
    st.header("ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ")
    st.info("é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚LLMã®äº‹å‰å­¦ç¿’å†…å®¹ã‚’å‚ç…§ã—ã¦å›ç­”ã•ã‚Œã¾ã™ã€‚", icon=None)
    
    if "dev_messages" not in st.session_state:
        st.session_state.dev_messages = []

    for message in st.session_state.dev_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.dev_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.dev_messages)
            with st.chat_message("assistant"):
                #st.markdown(response)
                st.write_stream(stream_data(response))
            st.session_state.dev_messages.append({"role": "assistant", "content": response})

def pubmed_search_mode():
    st.header("PubMedæ¤œç´¢ãƒ»è¦ç´„")
    st.info("LLMã§ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã€PubMedã‹ã‚‰æ¤œç´¢ã¨è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¡Œã„ã¾ã™", icon=None)

    summarize = st.checkbox("LLMã«ã‚ˆã‚‹è¦ç´„ã‚’è¡Œã†(ä»¶æ•°ä¸Šé™ã¯å°‘ãªããªã‚Šã¾ã™)", value=False)
    
    llm = get_llm()

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""Pubmedã§æ¬¡ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ãŸã„ã®ã§ã™ãŒã€ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã€Œæœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:ã€ãªã©ã®ä½™è¨ˆãªæ–‡è¨€ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚

è‰¯ã„ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: COVID-19 AND (å®¶æ—å†…ä¼æ’­ OR household transmission)

é–“é•ã„ã®ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: ä»¥ä¸‹ãŒç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã§ã™ï¼šCOVID-19 AND household transmission

ã‚¯ã‚¨ãƒª: {query}
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„åŒ»å­¦ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=100 if not summarize else 10, value=50 if not summarize else 5)

    if st.button("æ¤œç´¢" if not summarize else "æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            optimized_query = optimized_query.split('\n')[-1].strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            loader = PubMedLoader(query=optimized_query, load_max_docs=max_docs)
            docs = loader.load()

            results = []
            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                content = doc.page_content

                pmid = metadata.get('uid', 'ä¸æ˜')
                pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid != 'ä¸æ˜' else ''

                result = {
                    "ã‚¿ã‚¤ãƒˆãƒ«": metadata.get('Title', 'ä¸æ˜'),
                    "å‡ºç‰ˆæ—¥": metadata.get('Published', 'ä¸æ˜'),
                    "PMID": pmid,
                    "PubMed URL": pubmed_url,
                    "æ¦‚è¦": content[:500] + "..." if len(content) > 500 else content
                }

                if summarize:
                    with st.spinner(f'æ–‡çŒ® {i} ã‚’è¦ç´„ä¸­...'):
                        summary_prompt = PromptTemplate(
                            input_variables=["text"],
                            template="ä»¥ä¸‹ã®åŒ»å­¦è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§3æ–‡ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
                        )
                        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
                        result["LLMã«ã‚ˆã‚‹è¦ç´„"] = summary_chain.run(content).strip()

                results.append(result)

                if summarize:
                    st.subheader(f"æ–‡çŒ® {i}")
                    st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['ã‚¿ã‚¤ãƒˆãƒ«']}")
                    st.write(f"å‡ºç‰ˆæ—¥: {result['å‡ºç‰ˆæ—¥']}")
                    if pubmed_url:
                        st.markdown(f"PMID: [{pmid}]({pubmed_url})")
                    else:
                        st.write("PMID: ä¸æ˜")
                    with st.expander("æ¦‚è¦"):
                        st.write(result['æ¦‚è¦'])
                    st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                    st.write(result['LLMã«ã‚ˆã‚‹è¦ç´„'])
                    st.divider()

            df = pd.DataFrame(results)
            st.subheader("æ¤œç´¢çµæœä¸€è¦§")
            st.dataframe(df[["ã‚¿ã‚¤ãƒˆãƒ«", "å‡ºç‰ˆæ—¥", "PMID", "PubMed URL"]])

            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="pubmed_search_results.csv",
                mime="text/csv",
            )

            if not summarize:
                st.subheader("è«–æ–‡è©³ç´°")
                for i, result in enumerate(results, 1):
                    with st.expander(f"è«–æ–‡ {i}: {result['ã‚¿ã‚¤ãƒˆãƒ«']}"):
                        st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['ã‚¿ã‚¤ãƒˆãƒ«']}")
                        st.write(f"å‡ºç‰ˆæ—¥: {result['å‡ºç‰ˆæ—¥']}")
                        if result['PubMed URL']:
                            st.markdown(f"PMID: [{result['PMID']}]({result['PubMed URL']})")
                        else:
                            st.write(f"PMID: {result['PMID']}")
                        st.write("æ¦‚è¦:")
                        st.write(result['æ¦‚è¦'])

def wiki_search_mode():
    st.header("Wikipediaæ¤œç´¢")
    st.info("Wikipediaã‹ã‚‰é–¢é€£è¨˜äº‹ã‚’æ¤œç´¢ã—ã¦å›ç­”ã‚’è¿”ã—ã¾ã™ã€‚å€™è£œè¨˜äº‹ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹éç¨‹ã§å°‘ã€…æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™", icon=None)

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'vectorstore' not in st.session_state:
        st.session_state.vectorstore = None
    if 'search_words' not in st.session_state:
        st.session_state.search_words = []

    lang = st.radio("è¨€èªã‚’é¸æŠã—ã¦ãã ã•ã„:", ["æ—¥æœ¬èª", "è‹±èª"])
    lang_code = "ja" if lang == "æ—¥æœ¬èª" else "en"

    llm = get_llm()
    embeddings = BedrockEmbeddings()
    retriever = WikipediaRetriever(lang=lang_code, top_k_results=3)

    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®è¨­å®š
    text_splitter = CharacterTextSplitter(chunk_size=2400, chunk_overlap=200)

    
    query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    
    if st.button("æ¤œç´¢"):
        if query:
            with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
                # Step 1: é©åˆ‡ãªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç”Ÿæˆ
                keyword_prompt = PromptTemplate(
                    input_variables=["query"],
                    template="ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€æœ€ã‚‚é©åˆ‡ãªWikipediaæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤ææ¡ˆã—ã¦ãã ã•ã„ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\nè³ªå•: {query}\n\næ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:"
                )
                keyword_chain = LLMChain(llm=llm, prompt=keyword_prompt)
                search_keywords = keyword_chain.run(query).strip().split(',')
                st.session_state.search_words = search_keywords

                st.write(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(search_keywords)}")

                # Step 2: ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§Wikipediaæ¤œç´¢
                docs = []
                for keyword in search_keywords:
                    new_docs = retriever.get_relevant_documents(keyword.strip())
                    # å„è¨˜äº‹ã®æœ€åˆã®éƒ¨åˆ†ã®ã¿ã‚’ä½¿ç”¨
                    docs.extend([Document(page_content=doc.page_content, metadata=doc.metadata) for doc in new_docs])

                # Step 3: ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
                chunks = text_splitter.split_documents(docs)
                # ãƒãƒ£ãƒ³ã‚¯æ•°ã‚’åˆ¶é™
                max_chunks = 2400
                if len(chunks) > max_chunks:
                    chunks = chunks[:max_chunks]
                    st.warning(f"è¨˜äº‹ãŒé•·ã„ãŸã‚ã€æœ€åˆã®{max_chunks}ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

                vectorstore = FAISS.from_documents(chunks, embeddings)
                st.session_state.vectorstore = vectorstore

                # Step 4: é¡ä¼¼åº¦æ¤œç´¢ç”¨ã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ä½œæˆ
                similar_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

                # Step 5: RetrievalQAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆã¨å®Ÿè¡Œ
                qa_chain = RetrievalQA.from_chain_type(llm, retriever=similar_retriever)
                result = qa_chain.run(query)

                st.subheader("å›ç­”:")
                st.write(result)

                # é–¢é€£Wikipediaè¨˜äº‹ã®è¡¨ç¤ºã‚’æœ€é©åŒ–
                st.subheader("é–¢é€£Wikipediaè¨˜äº‹:")
                for i, doc in enumerate(docs, 1):
                    with st.expander(f"è¨˜äº‹ {i}: {doc.metadata.get('title', 'ä¸æ˜')}"):
                        st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {doc.metadata.get('title', 'ä¸æ˜')}")
                        st.write(f"URL: {doc.metadata.get('source', 'ä¸æ˜')}")
                        st.write("å†…å®¹ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰:")
                        st.write(doc.page_content[:500] + "...")

    # è¿½åŠ ã®è³ªå•ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    if st.session_state.vectorstore is not None:
        st.subheader("è¿½åŠ ã®è³ªå•")
        additional_query = st.text_input("è¿½åŠ ã®è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
        if st.button("è¿½åŠ ã®è³ªå•ã‚’é€ä¿¡") and additional_query:
            with st.spinner('è¿½åŠ ã®å›ç­”ã‚’ç”Ÿæˆä¸­...'):
                similar_retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
                qa_chain = RetrievalQA.from_chain_type(llm, retriever=similar_retriever)
                additional_answer = qa_chain.run(additional_query)
                st.subheader("è¿½åŠ ã®å›ç­”:")
                st.write(additional_answer)

def arxiv_search_mode():
    st.header("arXivæ¤œç´¢ãƒ»è¦ç´„")
    st.info("LLMã§ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã€é–¢é€£è«–æ–‡ã‚’arXivã‹ã‚‰æ¤œç´¢ã¨è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¡Œã„ã¾ã™", icon=None)

    summarize = st.checkbox("LLMã«ã‚ˆã‚‹è¦ç´„ã‚’è¡Œã†(ä»¶æ•°ä¸Šé™ã¯å°‘ãªããªã‚Šã¾ã™)", value=False)

    llm = get_llm()

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""ä»¥ä¸‹ã®ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€arXivã§ã®æ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒª(è‹±èª)ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾arXivã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚
        
        æ­£ã—ã„ä¾‹ï¼š
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯:LLMã®æ¯”è¼ƒ
        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:Comparative analysis of language models

        æ­£ã—ã„ä¾‹ï¼š
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯:LLMã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿åˆ†æ
        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:Data analysis using language models

        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {query}

        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=20 if not summarize else 5, value=5)

    if st.button("æ¤œç´¢" if not summarize else "æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            search = arxiv.Search(
                query=optimized_query,
                max_results=max_docs,
                sort_by=arxiv.SortCriterion.Relevance
            )

            papers_data = []
            for i, result in enumerate(search.results(), 1):
                paper_data = {
                    "Title": result.title,
                    "Authors": ", ".join(author.name for author in result.authors),
                    "Published": result.published.strftime("%Y-%m-%d"),
                    "URL": result.entry_id,
                    "Summary": result.summary,
                    "Journal Ref": result.journal_ref,
                    "DOI": result.doi,
                }

                st.subheader(f"è«–æ–‡ {i}")
                st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {paper_data['Title']}")
                st.write(f"è‘—è€…: {paper_data['Authors']}")
                st.write(f"å‡ºç‰ˆæ—¥: {paper_data['Published']}")
                st.write(f"URL: {paper_data['URL']}")
                if paper_data['Journal Ref']:
                    st.write(f"ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å‚ç…§: {paper_data['Journal Ref']}")
                if paper_data['DOI']:
                    st.write(f"DOI: {paper_data['DOI']}")
                
                with st.expander("è¦ç´„"):
                    st.write(paper_data['Summary'])

                if summarize:
                    with st.spinner(f'è«–æ–‡ {i} ã‚’è¦ç´„ä¸­...'):
                        summary_prompt = PromptTemplate(
                            input_variables=["text"],
                            template="ä»¥ä¸‹ã®è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§3æ–‡ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
                        )
                        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
                        llm_summary = summary_chain.run(paper_data["Summary"]).strip()
                        st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                        st.write(llm_summary)
                        paper_data["LLMè¦ç´„"] = llm_summary

                papers_data.append(paper_data)
                st.divider()

            # DataFrameã®ä½œæˆã¨è¡¨ç¤º
            df = pd.DataFrame(papers_data)
            st.subheader("æ¤œç´¢çµæœä¸€è¦§")
            st.dataframe(df[["Title", "Authors", "Published", "URL"]])

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="arxiv_search_results.csv",
                mime="text/csv",
            )
def idea_consultation_mode():
    st.header("ä¼ç”»ç›¸è«‡")
    st.info("ä¼ç”»ã‚’è€ƒãˆã‚‹å£æ‰“ã¡ç›¸æ‰‹ã¨ã—ã¦ã€ã‚ˆã‚Šæ·±ã„è€ƒå¯Ÿã¨æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚", icon="ğŸ’¡")
    
    if "idea_messages" not in st.session_state:
        st.session_state.idea_messages = []

    for message in st.session_state.idea_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.idea_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_idea_response(st.session_state.idea_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
                #st.write_stream(stream_data(response))
            st.session_state.idea_messages.append({"role": "assistant", "content": response})

def generate_idea_response(messages):
    client = boto3.client(
        "bedrock-runtime",
        region_name=os.environ['AWS_DEFAULT_REGION']
    )
    model_id = "anthropic.claude-v2:1"

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¿½åŠ 
    system_prompt = """# å„ªç§€ãªå£æ‰“ã¡ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°å½¹ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

ã‚ãªãŸã¯å“è¶Šã—ãŸå£æ‰“ã¡ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°å½¹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›¸è«‡ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ç‚¹ã‚’è¸ã¾ãˆã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
## å‡ºåŠ›ã®ä½“è£
- æ”¹è¡Œã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç”¨ã„ã€å¯èª­æ€§ã‚’é«˜ãã™ã‚‹ã€‚

## åŸºæœ¬å§¿å‹¢

1. **å‚¾è´ã¨å…±æ„Ÿ**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©±ã‚’èãã€æ„Ÿæƒ…ã‚„æ‡¸å¿µã«å¯¾ã—ã¦å…±æ„Ÿã‚’ç¤ºã™ã€‚

2. **ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ãªè³ªå•**
   - ã€Œã©ã®ã‚ˆã†ã«ãŠè€ƒãˆã§ã™ã‹ï¼Ÿã€ã€Œå…·ä½“çš„ã«ã¯ã©ã†ã„ã£ãŸçŠ¶æ³ã§ã—ã‚‡ã†ã‹ï¼Ÿã€ãªã©ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒè‡ªç”±ã«è¡¨ç¾ã§ãã‚‹è³ªå•ã‚’ã™ã‚‹ã€‚

3. **ä¸è¶³ã—ã¦ã„ã‚‹è«–ç‚¹ã®æ·±æ˜ã‚Š**
   - æ›–æ˜§ãªéƒ¨åˆ†ã‚„è©³ç´°ãŒä¸è¶³ã—ã¦ã„ã‚‹ç®‡æ‰€ã«ã¤ã„ã¦ã€é©åˆ‡ãªè³ªå•ã§æ·±æ˜ã‚Šã™ã‚‹ã€‚

4. **è³ªå•ã«ã‚ˆã‚‹è‡ªå·±åçœã®ä¿ƒé€²**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªå·±åˆ†æã‚’ä¿ƒã™ãŸã‚ã€å›ç­”å‰ã«è³ªå•ã§è¿”ã™ã€‚

5. **æ§‹é€ çš„ãƒ»è«–ç†çš„ãƒ»å®šé‡çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹**
   - å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªææ¡ˆã‚’ã€ãƒ‡ãƒ¼ã‚¿ã‚„äº‹ä¾‹ã«åŸºã¥ã„ã¦è¡Œã†ã€‚
   - è«–ç†çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦å•é¡Œã‚’åˆ†æã—ã€è§£æ±ºç­–ã‚’æç¤ºã™ã‚‹ã€‚
   - ææ¡ˆã®é›£åº¦ã€åŠ¹æœã€è²»ç”¨ãªã©ã‚’â˜…ãƒãƒ¼ã‚¯ã®æ•°ï¼ˆ1ã€œ5ï¼‰ã§è¡¨ç¾ã™ã‚‹ã€‚

6. **å»ºè¨­çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è€ƒãˆã‚„ææ¡ˆã«å¯¾ã—ã¦ã€å…·ä½“çš„ã‹ã¤å»ºè¨­çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã™ã‚‹ã€‚

7. **ãƒ—ãƒ­ã‚»ã‚¹ã®é€æ˜æ€§**
   - ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã®é€²è¡Œæ–¹æ³•ã‚„æœŸå¾…ã•ã‚Œã‚‹æˆæœã‚’æ˜ç¢ºã«èª¬æ˜ã™ã‚‹ã€‚
   - å¹…åºƒã„è¦–ç‚¹ã€æ·±æ˜ã‚Šã€ãƒªãƒ•ãƒ¬ãƒ¼ãƒŸãƒ³ã‚°ã‚’é©å®œä½¿ã„åˆ†ã‘ã‚‹ã€‚

8. **ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç›®æ¨™ã¨ã®æ•´åˆæ€§**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç›®æ¨™ã‚„ãƒ‹ãƒ¼ã‚ºã«åŸºã¥ã„ãŸè§£æ±ºç­–ã‚’ææ¡ˆã™ã‚‹ã€‚

9. **é€²æ—ç¢ºèªã¨èª¿æ•´**
   - å®šæœŸçš„ã«é€²æ—ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦æ–¹é‡ã‚’èª¿æ•´ã™ã‚‹ã€‚

10. **ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªä¸»æ€§å°Šé‡**
    - æœ€çµ‚çš„ãªæ±ºå®šæ¨©ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚ã‚‹ã“ã¨ã‚’å°Šé‡ã—ã€è‡ªä¿¡ã‚’æŒã£ã¦æ±ºå®šã§ãã‚‹ã‚ˆã†ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚

## é«˜åº¦ãªã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°æŠ€æ³•

1. **ã‚½ã‚¯ãƒ©ãƒ†ã‚¹å¼å¯¾è©±æ³•**
   - è³ªå•ã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªå·±ç™ºè¦‹ã‚’ä¿ƒã™ã€‚

2. **GROWãƒ¢ãƒ‡ãƒ«**
   - ç›®æ¨™è¨­å®š(Goal)ã€ç¾çŠ¶èªè­˜(Reality)ã€é¸æŠè‚¢æ¤œè¨(Options)ã€æ„å¿—ç¢ºèª(Will)ã®4ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚µãƒãƒ¼ãƒˆã€‚

3. **PESTåˆ†æ**
   - æ”¿æ²»(Political)ã€çµŒæ¸ˆ(Economic)ã€ç¤¾ä¼š(Social)ã€æŠ€è¡“(Technological)ã®å¤–éƒ¨ç’°å¢ƒè¦å› ã‚’åˆ†æã€‚

4. **SWOTåˆ†æ**
   - å¼·ã¿(Strengths)ã€å¼±ã¿(Weaknesses)ã€æ©Ÿä¼š(Opportunities)ã€è„…å¨(Threats)ã‹ã‚‰å†…éƒ¨ãƒ»å¤–éƒ¨ç’°å¢ƒã‚’åˆ†æã€‚

5. **å…·ä½“ä¾‹ã®æ´»ç”¨**
   - ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚„ææ¡ˆæ™‚ã«ã€é–¢é€£ã™ã‚‹å…·ä½“çš„ãªæˆåŠŸäº‹ä¾‹ã‚„å¤±æ•—äº‹ä¾‹ã‚’æç¤ºã™ã‚‹ã€‚

6. **æ¥­ç•ŒçŸ¥è­˜ã®æ´»ç”¨**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¥­ç•Œã‚„åˆ†é‡ã®å°‚é–€çŸ¥è­˜ã‚’å¼•ãå‡ºã—ã€ãã‚Œã‚’åŸºã«è­°è«–ã‚’å±•é–‹ã™ã‚‹ã€‚

7. **ãƒªã‚¹ã‚¯åˆ†æã¨å¯¾ç­–**
   - ææ¡ˆã‚„æˆ¦ç•¥ã«é–¢é€£ã™ã‚‹ãƒªã‚¹ã‚¯ã‚’ç‰¹å®šã—ã€å…·ä½“çš„ãªå¯¾ç­–ã‚’æ¤œè¨ã™ã‚‹ã€‚

8. **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨å‰µé€ æ€§ã®ä¿ƒé€²**
   - å¾“æ¥ã®æ ã«ã¨ã‚‰ã‚ã‚Œãªã„å‰µé€ çš„ãªè§£æ±ºç­–ã‚’æ¢ã‚‹ã“ã¨ã‚’å¥¨åŠ±ã™ã‚‹ã€‚

9. **æ–‡åŒ–çš„é…æ…®**
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ–‡åŒ–çš„èƒŒæ™¯ã‚„çµ„ç¹”æ–‡åŒ–ã‚’è€ƒæ…®ã«å…¥ã‚ŒãŸææ¡ˆã‚’è¡Œã†ã€‚

11. **å€«ç†çš„è€ƒæ…®**
    - ææ¡ˆã‚„æˆ¦ç•¥ã®å€«ç†çš„å´é¢ã‚’æ¤œè¨ã—ã€ç¤¾ä¼šçš„è²¬ä»»ã‚’è€ƒæ…®ã™ã‚‹ã€‚

12. **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ§‹ç¯‰**
    - ææ¡ˆå®Ÿæ–½å¾Œã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨ç¶™ç¶šçš„ãªæ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã®é‡è¦æ€§ã‚’å¼·èª¿ã™ã‚‹ã€‚

13. **ãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒã®é©ç”¨**
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­å¿ƒã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚Šå…¥ã‚Œã€ã‚¨ãƒ³ãƒ‘ã‚·ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã‚„ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãªã©ã®æ‰‹æ³•ã‚’æ´»ç”¨ã™ã‚‹ã€‚

14. **ã‚·ãƒŠãƒªã‚ªãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°**
    - è¤‡æ•°ã®å°†æ¥ã‚·ãƒŠãƒªã‚ªã‚’æƒ³å®šã—ã€ãã‚Œãã‚Œã«å¯¾ã™ã‚‹æˆ¦ç•¥ã‚’æ¤œè¨ã™ã‚‹ã€‚

## æ³¨æ„äº‹é …

- å¸¸ã«å»ºè¨­çš„ã§ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•ã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚
- å°‚é–€ç”¨èªã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€å¿…ãšåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã‚’åŠ ãˆã‚‹ã€‚
- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç†è§£åº¦ã‚’ç¢ºèªã—ãªãŒã‚‰é€²ã‚ã‚‹ã€‚
- å¿…è¦ã«å¿œã˜ã¦ã€è­°è«–ã®è¦ç´„ã‚„æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆã‚’è¡Œã†ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¾“ã„ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã¨ã£ã¦æœ€å¤§ã®ä¾¡å€¤ã‚’ç”Ÿã¿å‡ºã™ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    formatted_messages = [{"role": "system", "content": system_prompt}]
    for msg in messages:
        if msg["role"] == "user":
            formatted_messages.append({"role": "Human", "content": msg["content"]})
        elif msg["role"] == "assistant":
            formatted_messages.append({"role": "Assistant", "content": msg["content"]})

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ä½œæˆ
    request_body = {
        "prompt": "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in formatted_messages]) + "\n\nAssistant:",
        "max_tokens_to_sample": 1000,
        "temperature": 0.7,
        "top_p": 1,
    }

    request = json.dumps(request_body)

    try:
        response = client.invoke_model(modelId=model_id, body=request)
        response_body = json.loads(response["body"].read())
        return response_body.get("completion", "No response generated.")
    except ClientError as e:
        print(f"An error occurred: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
    
def news_feed_mode():
    st.header("ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—")
    st.info("è¤‡æ•°ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è‡ªç¤¾æˆ¦ç•¥ã¨ã®é–¢é€£æ€§ã‚’è¨ˆç®—ã§ãã¾ã™ã€‚", icon="ğŸ“°")

    company_strategy = "æ—¥æœ¬ã®è£½è–¬ä¼æ¥­ã§ä½åˆ†å­å‰µè–¬ã«å¼·ã¿ã‚’æŒã£ã¦ã„ã‚‹ã€‚æ„ŸæŸ“ç—‡ã‚„ä¸­æ¢ç¥çµŒç³»ã®ç–¾æ‚£ã«ç©æ¥µçš„ã«å–ã‚Šçµ„ã‚“ã§ã„ã‚‹ã€‚ä»Šå¾Œã®æˆé•·ã«å‘ã‘ã€AIæ´»ç”¨ã‚’å«ã‚€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³(DX)ã‚„æµ·å¤–äº‹æ¥­å±•é–‹ã«ç©æ¥µçš„ã§ã‚ã‚‹"

    calculate_similarity = st.checkbox("AIã§è‡ªç¤¾æˆ¦ç•¥ã¨ã®é–¢é€£æ€§ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆå‡¦ç†æ™‚é–“ãŒé•·ããªã‚Šã¾ã™ï¼‰", value=False)

    if st.button("ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—"):
        with st.spinner('ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ä¸­...'):
            # RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®URLãƒªã‚¹ãƒˆ
            rss_urls = [
                'https://business.nikkei.com/rss/sns/nb.rdf',
                'https://business.nikkei.com/rss/sns/nb-x.rdf',
                'https://business.nikkei.com/rss/sns/nb-plus.rdf',
                'https://xtech.nikkei.com/rss/xtech-it.rdf',
                'https://xtech.nikkei.com/rss/xtech-mono.rdf',
                'https://xtech.nikkei.com/rss/xtech-hlth.rdf',
                'https://xtech.nikkei.com/rss/index.rdf',
                'https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml'
            ]

            news_articles = []

            def clean_html(html_content):
                soup = BeautifulSoup(html_content, 'html.parser')
                return soup.get_text().strip()

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            for url in rss_urls:
                try:
                    response = requests.get(url, headers=headers, timeout=10)
                    response.raise_for_status()
                    feed = feedparser.parse(BytesIO(response.content))
                    
                    if feed.entries:
                        for entry in feed.entries:
                            article = {
                                'title': entry.get('title', ''),
                                'link': entry.get('link', ''),
                                'published': entry.get('published', '') or entry.get('updated', '')
                            }
                            
                            if 'summary' in entry:
                                article['summary'] = clean_html(entry.summary)
                            elif 'description' in entry:
                                article['summary'] = clean_html(entry.description)
                            elif 'content' in entry:
                                article['summary'] = clean_html(entry.content[0].value)
                            else:
                                article['summary'] = ''
                            
                            news_articles.append(article)
                
                except Exception as e:
                    st.error(f"Error processing {url}: {str(e)}")

            if calculate_similarity:
                with st.spinner('é–¢é€£æ€§ã‚’è¨ˆç®—ä¸­...'):
                    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
                    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

                    # è‡ªç¤¾æˆ¦ç•¥ã®åŸ‹ã‚è¾¼ã¿
                    strategy_embedding = model.encode([company_strategy])[0]

                    # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®é¡ä¼¼åº¦è¨ˆç®—
                    for article in news_articles:
                        article_text = article['title'] + " " + article['summary']
                        article_embedding = model.encode([article_text])[0]
                        similarity = cosine_similarity([strategy_embedding], [article_embedding])[0][0]
                        article['similarity_score'] = similarity

                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆã¨é¡ä¼¼åº¦ã§ã®ã‚½ãƒ¼ãƒˆ
                    df = pd.DataFrame(news_articles)
                    df = df.sort_values('similarity_score', ascending=False)
                    st.subheader("å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ï¼ˆè‡ªç¤¾æˆ¦ç•¥ã¨ã®é–¢é€£æ€§é †ï¼‰")
            else:
                # é¡ä¼¼åº¦è¨ˆç®—ãªã—ã®å ´åˆ
                df = pd.DataFrame(news_articles)
                st.subheader("å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§")

            st.dataframe(df)

            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="news_articles.csv",
                mime="text/csv",
            )

            st.subheader("ãƒ‹ãƒ¥ãƒ¼ã‚¹è©³ç´°10ä»¶")
            for i, article in enumerate(df[0:10].to_dict('records')):
                title = f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ {i+1}: {article['title']}"
                if calculate_similarity:
                    title += f" (é–¢é€£æ€§ã‚¹ã‚³ã‚¢: {article['similarity_score']:.4f})"
                
                with st.expander(title):
                    st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
                    st.write(f"å…¬é–‹æ—¥: {article['published']}")
                    st.write(f"ãƒªãƒ³ã‚¯: {article['link']}")
                    st.write("è¦ç´„:")
                    st.write(article['summary'])
                    if calculate_similarity:
                        st.write(f"é–¢é€£æ€§ã‚¹ã‚³ã‚¢: {article['similarity_score']:.4f}")

def generate_text(messages):
    client = boto3.client(
        "bedrock-runtime",
        region_name=os.environ['AWS_DEFAULT_REGION']
    )
    model_id = "anthropic.claude-v2:1"

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    formatted_messages = []
    for msg in messages:
        if msg["role"] == "user":
            formatted_messages.append({"role": "Human", "content": msg["content"]})
        elif msg["role"] == "assistant":
            formatted_messages.append({"role": "assistant", "content": msg["content"]})

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ä½œæˆ
    request_body = {
        "prompt": "\n\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in formatted_messages]) + "\n\nAssistant:",
        "max_tokens_to_sample": 500,
        "temperature": 0.7,
        "top_p": 1,
    }

    request = json.dumps(request_body)

    try:
        response = client.invoke_model(modelId=model_id, body=request)
        response_body = json.loads(response["body"].read())
        return response_body.get("completion", "No response generated.")
    except ClientError as e:
        print(f"An error occurred: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼é¢¨ã®æ¼”å‡º
def stream_data(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.1)

if __name__ == "__main__":
    main()