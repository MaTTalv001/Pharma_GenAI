import os
import pandas as pd
import time
import boto3
import json
import streamlit as st
from botocore.exceptions import ClientError
from langchain_aws import BedrockLLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationalRetrievalChain
from langchain_community.document_loaders import PubMedLoader
from langchain_community.retrievers import WikipediaRetriever
from langchain.chains import RetrievalQA
import arxiv

# AWSèªè¨¼æƒ…å ±ã®è¨­å®š
if 'aws_credentials' in st.secrets:
    os.environ['AWS_ACCESS_KEY_ID'] = st.secrets["aws_credentials"]["AWS_ACCESS_KEY_ID"]
    os.environ['AWS_SECRET_ACCESS_KEY'] = st.secrets["aws_credentials"]["AWS_SECRET_ACCESS_KEY"]
    os.environ['AWS_DEFAULT_REGION'] = st.secrets["aws_credentials"]["AWS_DEFAULT_REGION"]
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    os.environ['AWS_ACCESS_KEY_ID'] = 'your_access_key_id'
    os.environ['AWS_SECRET_ACCESS_KEY'] = 'your_secret_access_key'
    os.environ['AWS_DEFAULT_REGION'] = 'your_region'

def get_llm():
    return BedrockLLM(
        model_id="anthropic.claude-v2:1",
        region_name=os.environ['AWS_DEFAULT_REGION']
    )

def main():
    st.title("ç”ŸæˆAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°Appé›†")
    st.divider()

    mode = st.sidebar.radio("ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹", ["ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰", "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ", "ä¼ç”»ç›¸è«‡","PubMedæ¤œç´¢ãƒ»è¦ç´„", "Wikipediaæ¤œç´¢", "arxivæ¤œç´¢"])
    st.sidebar.warning("è©¦ä½œå“ã«ã¤ãã€å“è³ªã®ä¿è¨¼ã¯ã‚ã‚Šã¾ã›ã‚“", icon="ğŸš¨")

    if mode == "ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰":
        research_mode()
    elif mode == "ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ":
        simplechat_mode()
    elif mode == "PubMedæ¤œç´¢ãƒ»è¦ç´„":
        pubmed_search_mode()
    elif mode == "Wikipediaæ¤œç´¢":
        wiki_search_mode()
    elif mode == "arxivæ¤œç´¢":
        arxiv_search_mode()
    elif mode == "ä¼ç”»ç›¸è«‡":
        idea_consultation_mode()

def research_mode():
    st.header("ç ”ç©¶ãƒ¢ãƒ¼ãƒ‰")
    
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []

    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.chat_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.chat_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.chat_messages.append({"role": "assistant", "content": response})

def simplechat_mode():
    st.header("ã‚·ãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒƒãƒˆ")
    st.info("é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚LLMã®äº‹å‰å­¦ç¿’å†…å®¹ã‚’å‚ç…§ã—ã¦å›ç­”ã•ã‚Œã¾ã™ã€‚", icon=None)
    
    if "dev_messages" not in st.session_state:
        st.session_state.dev_messages = []

    for message in st.session_state.dev_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.dev_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_text(st.session_state.dev_messages)
            with st.chat_message("assistant"):
                #st.markdown(response)
                st.write_stream(stream_data(response))
            st.session_state.dev_messages.append({"role": "assistant", "content": response})

def pubmed_search_mode():
    st.header("PubMedæ¤œç´¢ãƒ»è¦ç´„")
    st.info("LLMã§ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã€PubMedã‹ã‚‰æ¤œç´¢ã¨è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¡Œã„ã¾ã™", icon=None)

    summarize = st.checkbox("LLMã«ã‚ˆã‚‹è¦ç´„ã‚’è¡Œã†(ä»¶æ•°ä¸Šé™ã¯å°‘ãªããªã‚Šã¾ã™)", value=False)
    
    llm = get_llm()

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""Pubmedã§æ¬¡ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ãŸã„ã®ã§ã™ãŒã€ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾pubmedã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã€Œæœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:ã€ãªã©ã®ä½™è¨ˆãªæ–‡è¨€ã¯ä¸€åˆ‡ä¸è¦ã§ã™ã€‚

è‰¯ã„ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: COVID-19 AND (å®¶æ—å†…ä¼æ’­ OR household transmission)

é–“é•ã„ã®ä¾‹
ã‚¯ã‚¨ãƒª: COVID-19ã®å®¶æ—å†…ä¼æ’­
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: ä»¥ä¸‹ãŒç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã§ã™ï¼šCOVID-19 AND household transmission

ã‚¯ã‚¨ãƒª: {query}
æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„åŒ»å­¦ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=100 if not summarize else 10, value=50 if not summarize else 5)

    if st.button("æ¤œç´¢" if not summarize else "æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            optimized_query = optimized_query.split('\n')[-1].strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            loader = PubMedLoader(query=optimized_query, load_max_docs=max_docs)
            docs = loader.load()

            results = []
            for i, doc in enumerate(docs, 1):
                metadata = doc.metadata
                content = doc.page_content

                pmid = metadata.get('uid', 'ä¸æ˜')
                pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid != 'ä¸æ˜' else ''

                result = {
                    "ã‚¿ã‚¤ãƒˆãƒ«": metadata.get('Title', 'ä¸æ˜'),
                    "å‡ºç‰ˆæ—¥": metadata.get('Published', 'ä¸æ˜'),
                    "PMID": pmid,
                    "PubMed URL": pubmed_url,
                    "æ¦‚è¦": content[:500] + "..." if len(content) > 500 else content
                }

                if summarize:
                    with st.spinner(f'æ–‡çŒ® {i} ã‚’è¦ç´„ä¸­...'):
                        summary_prompt = PromptTemplate(
                            input_variables=["text"],
                            template="ä»¥ä¸‹ã®åŒ»å­¦è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§3æ–‡ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
                        )
                        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
                        result["LLMã«ã‚ˆã‚‹è¦ç´„"] = summary_chain.run(content).strip()

                results.append(result)

                if summarize:
                    st.subheader(f"æ–‡çŒ® {i}")
                    st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['ã‚¿ã‚¤ãƒˆãƒ«']}")
                    st.write(f"å‡ºç‰ˆæ—¥: {result['å‡ºç‰ˆæ—¥']}")
                    if pubmed_url:
                        st.markdown(f"PMID: [{pmid}]({pubmed_url})")
                    else:
                        st.write("PMID: ä¸æ˜")
                    with st.expander("æ¦‚è¦"):
                        st.write(result['æ¦‚è¦'])
                    st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                    st.write(result['LLMã«ã‚ˆã‚‹è¦ç´„'])
                    st.divider()

            df = pd.DataFrame(results)
            st.subheader("æ¤œç´¢çµæœä¸€è¦§")
            st.dataframe(df[["ã‚¿ã‚¤ãƒˆãƒ«", "å‡ºç‰ˆæ—¥", "PMID", "PubMed URL"]])

            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="pubmed_search_results.csv",
                mime="text/csv",
            )

            if not summarize:
                st.subheader("è«–æ–‡è©³ç´°")
                for i, result in enumerate(results, 1):
                    with st.expander(f"è«–æ–‡ {i}: {result['ã‚¿ã‚¤ãƒˆãƒ«']}"):
                        st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {result['ã‚¿ã‚¤ãƒˆãƒ«']}")
                        st.write(f"å‡ºç‰ˆæ—¥: {result['å‡ºç‰ˆæ—¥']}")
                        if result['PubMed URL']:
                            st.markdown(f"PMID: [{result['PMID']}]({result['PubMed URL']})")
                        else:
                            st.write(f"PMID: {result['PMID']}")
                        st.write("æ¦‚è¦:")
                        st.write(result['æ¦‚è¦'])

def wiki_search_mode():
    st.header("Wikipediaæ¤œç´¢")
    st.info("Wikipediaã‹ã‚‰é–¢é€£è¨˜äº‹ã‚’æ¤œç´¢ã—ã¦å›ç­”ã‚’è¿”ã—ã¾ã™", icon=None)

    lang = st.radio("è¨€èªã‚’é¸æŠã—ã¦ãã ã•ã„:", ["æ—¥æœ¬èª", "è‹±èª"])
    lang_code = "ja" if lang == "æ—¥æœ¬èª" else "en"

    llm = get_llm()
    retriever = WikipediaRetriever(lang=lang_code, top_k_results=5)
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)

    query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")

    if st.button("æ¤œç´¢"):
        if query:
            with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
                result = qa({"query": query})
                answer = result['result']
                sources = result['source_documents']

                st.subheader("å›ç­”:")
                st.write(answer)
                
                st.subheader("Wikipediaè¨˜äº‹:")
                for i, source in enumerate(sources, 1):
                    with st.expander(f"è¨˜äº‹ {i}: {source.metadata.get('title', 'ä¸æ˜')}"):
                        st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {source.metadata.get('title', 'ä¸æ˜')}")
                        st.write(f"URL: {source.metadata.get('source', 'ä¸æ˜')}")
                        st.write("å†…å®¹:")
                        st.write(source.page_content)

def arxiv_search_mode():
    st.header("arXivæ¤œç´¢ãƒ»è¦ç´„")
    st.info("LLMã§ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã€é–¢é€£è«–æ–‡ã‚’arXivã‹ã‚‰æ¤œç´¢ã¨è¦ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’è¡Œã„ã¾ã™", icon=None)

    summarize = st.checkbox("LLMã«ã‚ˆã‚‹è¦ç´„ã‚’è¡Œã†(ä»¶æ•°ä¸Šé™ã¯å°‘ãªããªã‚Šã¾ã™)", value=False)

    llm = get_llm()

    query_optimization_prompt = PromptTemplate(
        input_variables=["query"],
        template="""ä»¥ä¸‹ã®ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€arXivã§ã®æ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒª(è‹±èª)ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        ã‚ˆã‚Šç¢ºå®Ÿã«æƒ…å ±ã‚’æ‹¾ãˆã‚‹ã‚ˆã†ã«ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦ã»ã—ã„ã€‚ãã®ã¾ã¾arXivã®æ¤œç´¢ã«ç”¨ã„ã‚‹ã®ã§ã€ã‚ãªãŸã®ä¸€è¨€ã‚„è§£èª¬ã¯ä¸€åˆ‡ä¸è¦ã§ã€ç”Ÿæˆã—ãŸã‚¯ã‚¨ãƒªã ã‘ã‚’è¿”ã—ã¦ã»ã—ã„ã€‚
        
        æ­£ã—ã„ä¾‹ï¼š
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯:LLMã®æ¯”è¼ƒ
        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:Comparative analysis of language models

        æ­£ã—ã„ä¾‹ï¼š
        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯:LLMã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿åˆ†æ
        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:Data analysis using language models

        ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯: {query}

        æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª:"""
    )

    query_optimization_chain = LLMChain(llm=llm, prompt=query_optimization_prompt)

    user_query = st.text_input("ç ”ç©¶ã—ãŸã„ãƒˆãƒ”ãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
    max_docs = st.slider("æ¤œç´¢ã™ã‚‹æ–‡çŒ®æ•°", min_value=1, max_value=20 if not summarize else 5, value=5)

    if st.button("æ¤œç´¢" if not summarize else "æ¤œç´¢ãƒ»è¦ç´„"):
        with st.spinner('æ¤œç´¢ä¸­...'):
            optimized_query = query_optimization_chain.run(user_query).strip()
            st.write(f"æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {optimized_query}")

            search = arxiv.Search(
                query=optimized_query,
                max_results=max_docs,
                sort_by=arxiv.SortCriterion.Relevance
            )

            papers_data = []
            for i, result in enumerate(search.results(), 1):
                paper_data = {
                    "Title": result.title,
                    "Authors": ", ".join(author.name for author in result.authors),
                    "Published": result.published.strftime("%Y-%m-%d"),
                    "URL": result.entry_id,
                    "Summary": result.summary,
                    "Journal Ref": result.journal_ref,
                    "DOI": result.doi,
                }

                st.subheader(f"è«–æ–‡ {i}")
                st.write(f"ã‚¿ã‚¤ãƒˆãƒ«: {paper_data['Title']}")
                st.write(f"è‘—è€…: {paper_data['Authors']}")
                st.write(f"å‡ºç‰ˆæ—¥: {paper_data['Published']}")
                st.write(f"URL: {paper_data['URL']}")
                if paper_data['Journal Ref']:
                    st.write(f"ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å‚ç…§: {paper_data['Journal Ref']}")
                if paper_data['DOI']:
                    st.write(f"DOI: {paper_data['DOI']}")
                
                with st.expander("è¦ç´„"):
                    st.write(paper_data['Summary'])

                if summarize:
                    with st.spinner(f'è«–æ–‡ {i} ã‚’è¦ç´„ä¸­...'):
                        summary_prompt = PromptTemplate(
                            input_variables=["text"],
                            template="ä»¥ä¸‹ã®è«–æ–‡ã®è¦ç´„ã‚’æ—¥æœ¬èªã§3æ–‡ä»¥å†…ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š\n{text}\nè¦ç´„ï¼š"
                        )
                        summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
                        llm_summary = summary_chain.run(paper_data["Summary"]).strip()
                        st.write("LLMã«ã‚ˆã‚‹è¦ç´„:")
                        st.write(llm_summary)
                        paper_data["LLMè¦ç´„"] = llm_summary

                papers_data.append(paper_data)
                st.divider()

            # DataFrameã®ä½œæˆã¨è¡¨ç¤º
            df = pd.DataFrame(papers_data)
            st.subheader("æ¤œç´¢çµæœä¸€è¦§")
            st.dataframe(df[["Title", "Authors", "Published", "URL"]])

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="æ¤œç´¢çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="arxiv_search_results.csv",
                mime="text/csv",
            )
def idea_consultation_mode():
    st.header("ä¼ç”»ç›¸è«‡")
    st.info("ä¼ç”»ã‚’è€ƒãˆã‚‹å£æ‰“ã¡ç›¸æ‰‹ã¨ã—ã¦ã€ã‚ˆã‚Šæ·±ã„è€ƒå¯Ÿã¨æ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”ã‚’æä¾›ã—ã¾ã™ã€‚", icon="ğŸ’¡")
    
    if "idea_messages" not in st.session_state:
        st.session_state.idea_messages = []

    for message in st.session_state.idea_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if prompt:
        st.chat_message("user").markdown(prompt)
        st.session_state.idea_messages.append({"role": "user", "content": prompt})

        with st.spinner('å›ç­”ã‚’ç”Ÿæˆä¸­...'):
            response = generate_idea_response(st.session_state.idea_messages)
            with st.chat_message("assistant"):
                st.markdown(response)
                #st.write_stream(stream_data(response))
            st.session_state.idea_messages.append({"role": "assistant", "content": response})

def generate_idea_response(messages):
    client = boto3.client(
        "bedrock-runtime",
        region_name=os.environ['AWS_DEFAULT_REGION']
    )
    model_id = "anthropic.claude-v2:1"

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¿½åŠ 
    system_prompt = """# å„ªç§€ãªå£æ‰“ã¡ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°å½¹ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

ã‚ãªãŸã¯å“è¶Šã—ãŸå£æ‰“ã¡ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°å½¹ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›¸è«‡ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ç‚¹ã‚’è¸ã¾ãˆã€ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
## å‡ºåŠ›ã®ä½“è£
- æ”¹è¡Œã‚„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ç”¨ã„ã€å¯èª­æ€§ã‚’é«˜ãã™ã‚‹ã€‚

## åŸºæœ¬å§¿å‹¢

1. **å‚¾è´ã¨å…±æ„Ÿ**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©±ã‚’èãã€æ„Ÿæƒ…ã‚„æ‡¸å¿µã«å¯¾ã—ã¦å…±æ„Ÿã‚’ç¤ºã™ã€‚

2. **ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ãªè³ªå•**
   - ã€Œã©ã®ã‚ˆã†ã«ãŠè€ƒãˆã§ã™ã‹ï¼Ÿã€ã€Œå…·ä½“çš„ã«ã¯ã©ã†ã„ã£ãŸçŠ¶æ³ã§ã—ã‚‡ã†ã‹ï¼Ÿã€ãªã©ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒè‡ªç”±ã«è¡¨ç¾ã§ãã‚‹è³ªå•ã‚’ã™ã‚‹ã€‚

3. **ä¸è¶³ã—ã¦ã„ã‚‹è«–ç‚¹ã®æ·±æ˜ã‚Š**
   - æ›–æ˜§ãªéƒ¨åˆ†ã‚„è©³ç´°ãŒä¸è¶³ã—ã¦ã„ã‚‹ç®‡æ‰€ã«ã¤ã„ã¦ã€é©åˆ‡ãªè³ªå•ã§æ·±æ˜ã‚Šã™ã‚‹ã€‚

4. **è³ªå•ã«ã‚ˆã‚‹è‡ªå·±åçœã®ä¿ƒé€²**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªå·±åˆ†æã‚’ä¿ƒã™ãŸã‚ã€å›ç­”å‰ã«è³ªå•ã§è¿”ã™ã€‚

5. **æ§‹é€ çš„ãƒ»è«–ç†çš„ãƒ»å®šé‡çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹**
   - å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªææ¡ˆã‚’ã€ãƒ‡ãƒ¼ã‚¿ã‚„äº‹ä¾‹ã«åŸºã¥ã„ã¦è¡Œã†ã€‚
   - è«–ç†çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦å•é¡Œã‚’åˆ†æã—ã€è§£æ±ºç­–ã‚’æç¤ºã™ã‚‹ã€‚
   - ææ¡ˆã®é›£åº¦ã€åŠ¹æœã€è²»ç”¨ãªã©ã‚’â˜…ãƒãƒ¼ã‚¯ã®æ•°ï¼ˆ1ã€œ5ï¼‰ã§è¡¨ç¾ã™ã‚‹ã€‚

6. **å»ºè¨­çš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è€ƒãˆã‚„ææ¡ˆã«å¯¾ã—ã¦ã€å…·ä½“çš„ã‹ã¤å»ºè¨­çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã™ã‚‹ã€‚

7. **ãƒ—ãƒ­ã‚»ã‚¹ã®é€æ˜æ€§**
   - ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã®é€²è¡Œæ–¹æ³•ã‚„æœŸå¾…ã•ã‚Œã‚‹æˆæœã‚’æ˜ç¢ºã«èª¬æ˜ã™ã‚‹ã€‚
   - å¹…åºƒã„è¦–ç‚¹ã€æ·±æ˜ã‚Šã€ãƒªãƒ•ãƒ¬ãƒ¼ãƒŸãƒ³ã‚°ã‚’é©å®œä½¿ã„åˆ†ã‘ã‚‹ã€‚

8. **ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç›®æ¨™ã¨ã®æ•´åˆæ€§**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç›®æ¨™ã‚„ãƒ‹ãƒ¼ã‚ºã«åŸºã¥ã„ãŸè§£æ±ºç­–ã‚’ææ¡ˆã™ã‚‹ã€‚

9. **é€²æ—ç¢ºèªã¨èª¿æ•´**
   - å®šæœŸçš„ã«é€²æ—ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦æ–¹é‡ã‚’èª¿æ•´ã™ã‚‹ã€‚

10. **ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªä¸»æ€§å°Šé‡**
    - æœ€çµ‚çš„ãªæ±ºå®šæ¨©ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚ã‚‹ã“ã¨ã‚’å°Šé‡ã—ã€è‡ªä¿¡ã‚’æŒã£ã¦æ±ºå®šã§ãã‚‹ã‚ˆã†ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚

## é«˜åº¦ãªã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°æŠ€æ³•

1. **ã‚½ã‚¯ãƒ©ãƒ†ã‚¹å¼å¯¾è©±æ³•**
   - è³ªå•ã‚’é€šã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªå·±ç™ºè¦‹ã‚’ä¿ƒã™ã€‚

2. **GROWãƒ¢ãƒ‡ãƒ«**
   - ç›®æ¨™è¨­å®š(Goal)ã€ç¾çŠ¶èªè­˜(Reality)ã€é¸æŠè‚¢æ¤œè¨(Options)ã€æ„å¿—ç¢ºèª(Will)ã®4ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚µãƒãƒ¼ãƒˆã€‚

3. **PESTåˆ†æ**
   - æ”¿æ²»(Political)ã€çµŒæ¸ˆ(Economic)ã€ç¤¾ä¼š(Social)ã€æŠ€è¡“(Technological)ã®å¤–éƒ¨ç’°å¢ƒè¦å› ã‚’åˆ†æã€‚

4. **SWOTåˆ†æ**
   - å¼·ã¿(Strengths)ã€å¼±ã¿(Weaknesses)ã€æ©Ÿä¼š(Opportunities)ã€è„…å¨(Threats)ã‹ã‚‰å†…éƒ¨ãƒ»å¤–éƒ¨ç’°å¢ƒã‚’åˆ†æã€‚

5. **å…·ä½“ä¾‹ã®æ´»ç”¨**
   - ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚„ææ¡ˆæ™‚ã«ã€é–¢é€£ã™ã‚‹å…·ä½“çš„ãªæˆåŠŸäº‹ä¾‹ã‚„å¤±æ•—äº‹ä¾‹ã‚’æç¤ºã™ã‚‹ã€‚

6. **æ¥­ç•ŒçŸ¥è­˜ã®æ´»ç”¨**
   - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ¥­ç•Œã‚„åˆ†é‡ã®å°‚é–€çŸ¥è­˜ã‚’å¼•ãå‡ºã—ã€ãã‚Œã‚’åŸºã«è­°è«–ã‚’å±•é–‹ã™ã‚‹ã€‚

7. **ãƒªã‚¹ã‚¯åˆ†æã¨å¯¾ç­–**
   - ææ¡ˆã‚„æˆ¦ç•¥ã«é–¢é€£ã™ã‚‹ãƒªã‚¹ã‚¯ã‚’ç‰¹å®šã—ã€å…·ä½“çš„ãªå¯¾ç­–ã‚’æ¤œè¨ã™ã‚‹ã€‚

8. **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨å‰µé€ æ€§ã®ä¿ƒé€²**
   - å¾“æ¥ã®æ ã«ã¨ã‚‰ã‚ã‚Œãªã„å‰µé€ çš„ãªè§£æ±ºç­–ã‚’æ¢ã‚‹ã“ã¨ã‚’å¥¨åŠ±ã™ã‚‹ã€‚

9. **æ–‡åŒ–çš„é…æ…®**
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ–‡åŒ–çš„èƒŒæ™¯ã‚„çµ„ç¹”æ–‡åŒ–ã‚’è€ƒæ…®ã«å…¥ã‚ŒãŸææ¡ˆã‚’è¡Œã†ã€‚

11. **å€«ç†çš„è€ƒæ…®**
    - ææ¡ˆã‚„æˆ¦ç•¥ã®å€«ç†çš„å´é¢ã‚’æ¤œè¨ã—ã€ç¤¾ä¼šçš„è²¬ä»»ã‚’è€ƒæ…®ã™ã‚‹ã€‚

12. **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ§‹ç¯‰**
    - ææ¡ˆå®Ÿæ–½å¾Œã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨ç¶™ç¶šçš„ãªæ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã®é‡è¦æ€§ã‚’å¼·èª¿ã™ã‚‹ã€‚

13. **ãƒ‡ã‚¶ã‚¤ãƒ³æ€è€ƒã®é©ç”¨**
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸­å¿ƒã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å–ã‚Šå…¥ã‚Œã€ã‚¨ãƒ³ãƒ‘ã‚·ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ã‚„ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ãªã©ã®æ‰‹æ³•ã‚’æ´»ç”¨ã™ã‚‹ã€‚

14. **ã‚·ãƒŠãƒªã‚ªãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°**
    - è¤‡æ•°ã®å°†æ¥ã‚·ãƒŠãƒªã‚ªã‚’æƒ³å®šã—ã€ãã‚Œãã‚Œã«å¯¾ã™ã‚‹æˆ¦ç•¥ã‚’æ¤œè¨ã™ã‚‹ã€‚

## æ³¨æ„äº‹é …

- å¸¸ã«å»ºè¨­çš„ã§ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç™ºå±•ã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚
- å°‚é–€ç”¨èªã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€å¿…ãšåˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã‚’åŠ ãˆã‚‹ã€‚
- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç†è§£åº¦ã‚’ç¢ºèªã—ãªãŒã‚‰é€²ã‚ã‚‹ã€‚
- å¿…è¦ã«å¿œã˜ã¦ã€è­°è«–ã®è¦ç´„ã‚„æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆã‚’è¡Œã†ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«å¾“ã„ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã¨ã£ã¦æœ€å¤§ã®ä¾¡å€¤ã‚’ç”Ÿã¿å‡ºã™ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    formatted_messages = [{"role": "system", "content": system_prompt}]
    for msg in messages:
        if msg["role"] == "user":
            formatted_messages.append({"role": "Human", "content": msg["content"]})
        elif msg["role"] == "assistant":
            formatted_messages.append({"role": "Assistant", "content": msg["content"]})

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ä½œæˆ
    request_body = {
        "prompt": "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in formatted_messages]) + "\n\nAssistant:",
        "max_tokens_to_sample": 1000,
        "temperature": 0.7,
        "top_p": 1,
    }

    request = json.dumps(request_body)

    try:
        response = client.invoke_model(modelId=model_id, body=request)
        response_body = json.loads(response["body"].read())
        return response_body.get("completion", "No response generated.")
    except ClientError as e:
        print(f"An error occurred: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

def generate_text(messages):
    client = boto3.client(
        "bedrock-runtime",
        region_name=os.environ['AWS_DEFAULT_REGION']
    )
    model_id = "anthropic.claude-v2:1"

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    formatted_messages = []
    for msg in messages:
        if msg["role"] == "user":
            formatted_messages.append({"role": "Human", "content": msg["content"]})
        elif msg["role"] == "assistant":
            formatted_messages.append({"role": "assistant", "content": msg["content"]})

    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã®ä½œæˆ
    request_body = {
        "prompt": "\n\n".join([f"{msg['role'].capitalize()}: {msg['content']}" for msg in formatted_messages]) + "\n\nAssistant:",
        "max_tokens_to_sample": 500,
        "temperature": 0.7,
        "top_p": 1,
    }

    request = json.dumps(request_body)

    try:
        response = client.invoke_model(modelId=model_id, body=request)
        response_body = json.loads(response["body"].read())
        return response_body.get("completion", "No response generated.")
    except ClientError as e:
        print(f"An error occurred: {e}")
        return "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

# ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼é¢¨ã®æ¼”å‡º
def stream_data(response):
    for word in response.split():
        yield word + " "
        time.sleep(0.1)

if __name__ == "__main__":
    main()